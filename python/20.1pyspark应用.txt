1、单词计数：读取文件内的数据，统计文件内各单词的出现次数
rdd=sc.textFile("D:/python/data/word.txt")
rdd1=rdd.flatMap(lambda x:x.split(" "))#按照空格进行切分，因为每行是一个列表，需要解嵌套形成一个大列表包含所有的word，所以用flatmap
#现在要做一个二元元组(word,1)表示该word出现了1次，之后用reducebykey方法将值1相加，即为总次数
word_rdd=rdd1.map(lambda word:(word,1)) #函数可以返回一个元组
word_result=word_rdd.reduceByKey(lambda a,b:a+b)
print(word_result.collect())  
输出[('abc', 2), ('ab', 2), ('b', 1), ('c', 1), ('bc', 1), ('abcd', 1), ('ac', 3), ('a', 1)]

2、给定一组数据，该数据由多个有相同key值的字典组成（json数据），但该数据的每行可能有多个字典，字典间用|分隔；字典包括：id、timestamp、category、areaname、money，分别为订单号、时间、商品类别、城市、销售额
需求：
#按照从大到小对各个城市的销售额进行排名、
file_rdd=sc.textFile("D:/python/data.txt")#从文件中读取数据
json_str_rdd=file_rdd.flatMap(lambda x:x.split("|"))#切分返回一个列表，列表里是json型字符串
json_rdd=json_str_rdd.map(lambda  x:json.loads(x))#将json型字符串转为json量，得到（{},{},{})类型的rdd对象
city_money_rdd=json_rdd.map(lambda x:(x['areaName'],int(x['money'])))#提取出城市名和销售额，并将其转为二元元组形式
city_money_result=city_money_rdd.reduceByKey(lambda a,b:a+b)#分组聚合
city_money_sort_result=city_money_result.sortBy(lambda x:x[1],ascending=False,numPartitions=1)#根据第二个量（销售额）进行降序排序
print(city_money_sort_result)
#统计全部城市有哪些商品类别
category_rdd=json_rdd.map(lambda x:x['category']).distinct()#链式调用,提取category并去重
print(category_rdd.collect())
#统计北京市有哪些商品类别
bj_json_rdd=json_rdd.filter(lambda x:x['areaName'=='北京'])#过滤，得到北京的json数据
bj_category_rdd=bj_json_rdd.map(lambda x:x['category']).distinct()
print(bj_category_rdd.collect())

3、给定一组数据，数据内每行格式为：搜索时间（时：分：秒） 搜索用户id 搜索关键词 ，其中列和列之前用制表符tab分隔，时间类似于23：00：00形式
（1）输出热门搜索时间段（精确到小时）top3 -本质上就是单词计数
#分行写代码-链式调用：pycharm中直接在.后回车，或者加上\后回车
#先按制表符分隔数据，再取到时间（行列表中第一个元素的前两位），再变成二元元组（同单词计数），再分组聚合并排序，最后取前3位输出。其中前3个map可以合成成1个
file_rdd=sc.textFile("D:/python/data.txt")
result=file_rdd.map(lambda x:x.split("\t")).\ 
    map(lambda x:x[0][:2]).\
    map(lambda x:(x,1)).\
    reduceByKey(lambda a,b:a+b).\
    sortBy(lambda x:x[1],ascending=False,numPartitions=1).\
    take(3)
（2）统计热门搜索关键词top3
#分隔数据，取到第二个元素，变成二元元组，分组聚合，排序，取前3个
result=file_rdd.map(lambda x:(x.split("\t")[2],1)).\
    reduceByKey(lambda a,b:a+b).\
    sortBy(lambda x:x[1],ascending=False,numPartitions=1).\
    take(3)
（3）统计给定关键词在哪个搜索时段出现最多
#分隔数据，过滤数据，只留下有给定关键词的数据，再使用（1）中的方法
result=file_rdd.map(lambda x:x.split("\t")).\
    filter(lambda x:x[2]=='abc').\
    map(lambda x:(x[0][:2],1)).\
    reduceByKey(lambda a,b:a+b).\
    sortBy(lambda x:x[1],ascending=False,numPartitions=1).\
    take(1)
（4）将原数据转换成json模式输出
#先分隔数据，再把分隔好的每段数据都对应一个key（key分别设为time、user_id、key_word）,转换成字典（json模式），输出到某文件中
file_rdd.map(lambda x:x.split("\t")).\
    map(lambda x:{"time":x[0],"user_id":x[1],"key_word":x[2]}).\
    saveAsTextFile("D:/python/data")


